{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "yl-PKGEcT4Rm",
      "metadata": {
        "id": "yl-PKGEcT4Rm"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"http://wandb.me/logo-im-png\" width=\"800\" alt=\"Weights & Biases\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ae8d5f",
      "metadata": {
        "id": "e6ae8d5f"
      },
      "source": [
        "# ü™ê Operation REBOOT: Mission Start\n",
        "\n",
        "Welcome, **Neural Architect**. The ship's AI core is down. Your job: fine-tune a foundational model with astrological Q&A data to restore its deep space reasoning abilities.\n",
        "\n",
        "**Your mission:**\n",
        "- Adjust dataset splits\n",
        "- Configure training arguments\n",
        "- Launch training and monitor with **Weights & Biases (W&B)**\n",
        "- Test and evaluate your fine-tuned model\n",
        "\n",
        "All systems go. Let's bring this vessel back online."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z4hJWD_fQ_CA",
      "metadata": {
        "id": "Z4hJWD_fQ_CA"
      },
      "source": [
        "#### Install and Import  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "udsFRNPeQ-WY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udsFRNPeQ-WY",
        "outputId": "20101bfd-1aa4-4f0f-ece2-0ed51d41503e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/348.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate wandb trl huggingface_hub -q\n",
        "!pip install bitsandbytes peft -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lcd7lpzkRH5a",
      "metadata": {
        "id": "Lcd7lpzkRH5a"
      },
      "outputs": [],
      "source": [
        "from utilities.helpers import *\n",
        "import wandb\n",
        "\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel,\n",
        ")\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e022b8ad",
      "metadata": {
        "id": "e022b8ad"
      },
      "source": [
        "## üîå Connect Neural Telemetry (W&B Setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c79fd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5c79fd6",
        "outputId": "bc53d709-9eb1-419a-cdda-9bac5d1ff6ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammadbakir\u001b[0m (\u001b[33mm-bakir\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# When prompted to authorize your\n",
        "# wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
        "# wandb: Paste an API key from your profile and hit enter:\n",
        "\n",
        "#Login to W&B - If not using Colab to store/fetch key comment next two lines and just run wandb.login() command\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_key)\n",
        "#wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2252cad",
      "metadata": {
        "id": "b2252cad"
      },
      "outputs": [],
      "source": [
        "WANDB_PROJECT_NAME = \"Astros-FT-Workshop\"\n",
        "WANDB_ENTITY = \"FT-Testing\" #Set your W&B Entity #TODO - Change to None for final release\n",
        "WORKSHOP_TEAM_NAME = \"Wandb_Crew\" #Set to your Workshop Team Name #TODO - Change to None for final release"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67257b8",
      "metadata": {
        "id": "b67257b8"
      },
      "source": [
        "## üß™ Dataset Control Room\n",
        "Adjust the dataset splits and prepare the astrological QA dataset for training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f8b93f",
      "metadata": {
        "id": "e3f8b93f"
      },
      "source": [
        "## üåé Initialize Experiment, Read Data, Split Data ‚òÑÔ∏è\n",
        "\n",
        "In this section, we:\n",
        "\n",
        "* Retrieve the Astros Dataset from [W&B Registry FC_FT_Workshop_Dataset collection](https://wandb.ai/orgs/FullyConnected-2025-Workshops/registry/dataset?selectionPath=fullyconnected-2025-workshops%2Fwandb-registry-dataset%2FFC_FT_Workshop_Dataset&view=versions) TODO: Update for final version\n",
        "* Load the Astros Dataset containing universe-related Q&A data.\n",
        "* Create prompts from the question/answer pairs & load into a pandas dataframe\n",
        "* Convert the pandas DataFrame into a Hugging Face Dataset.\n",
        "\n",
        "‚úÖ All the heavy lifting is done here automatically ‚Äî no manual setup needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tIPyPmnLkr__",
      "metadata": {
        "id": "tIPyPmnLkr__"
      },
      "source": [
        "#### Load Notebook Animations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_UNRuIWxkp19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_UNRuIWxkp19",
        "outputId": "f8105a2a-81a0-4ea8-d12c-63a02fb0cd8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1 of 2 files downloaded...\r\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n"
          ]
        }
      ],
      "source": [
        "#helper\n",
        "#Load Animations\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                  project=WANDB_PROJECT_NAME,\n",
        "                  job_type=\"animation_retrieval\",\n",
        "                  name=f\"fetch_animations\",\n",
        "                  settings=wandb.Settings(silent=True)\n",
        "                  )\n",
        "\n",
        "# Download Animations\n",
        "animate_artifact = run.use_artifact('wandb-registry-FT-Workshop-Collatoral/Animation-Scripts:v1', type='code')\n",
        "animate_dir = animate_artifact.download()\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5b3adb",
      "metadata": {
        "id": "4c5b3adb"
      },
      "source": [
        "#### Helper functions to load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27c9adf",
      "metadata": {
        "id": "e27c9adf"
      },
      "outputs": [],
      "source": [
        "#helper\n",
        "# Load and validate the JSONL dataset\n",
        "def load_jsonl_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Load a JSONL dataset into a pandas DataFrame.\n",
        "\n",
        "    Each line is parsed as a JSON object.\n",
        "    Handles and reports JSON decoding errors without stopping execution.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the JSONL file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing the loaded dataset.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    error_count = 0\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"[Error] Line {i}: Could not decode JSON.\")\n",
        "                    print(f\"Content: {line}\")\n",
        "                    print(f\"Error: {str(e)}\\n\")\n",
        "                    error_count += 1\n",
        "                    continue\n",
        "    if error_count > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è Finished loading with {error_count} decoding errors. Please check your dataset formatting!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Successfully loaded dataset with no errors.\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create a prompt format from each question-answer pair\n",
        "def create_prompt(row):\n",
        "    return f\"Question: {row['question']}\\nAnswer: {row['answer']}\"\n",
        "\n",
        "# Utility function to reload and prepare the dataset when needed\n",
        "def reload_astros_dataset(artifact_dir, filename=\"astro_dataset_train.jsonl\"):\n",
        "    \"\"\"\n",
        "    Reload the Astros dataset from a downloaded artifact directory.\n",
        "\n",
        "    Args:\n",
        "        artifact_dir (str or Path): Path to the artifact directory.\n",
        "        filename (str): Name of the JSONL dataset file. Defaults to \"astro_dataset_train.jsonl\".\n",
        "\n",
        "    Returns:\n",
        "        tuple: A pandas DataFrame and a Hugging Face Dataset ready for training.\n",
        "    \"\"\"\n",
        "    dataset_path = Path(artifact_dir) / filename\n",
        "    df = load_jsonl_dataset(str(dataset_path))\n",
        "    df['text'] = df.apply(create_prompt, axis=1)\n",
        "    return df, Dataset.from_pandas(df)\n",
        "\n",
        "def load_and_prepare_dataset(artifact_dir, filename, dataset_type=\"dataset\"):\n",
        "    \"\"\"\n",
        "    Load a JSONL dataset, create prompts, and convert to Hugging Face Dataset format.\n",
        "\n",
        "    Args:\n",
        "        artifact_dir (str or Path): Path to the artifact directory\n",
        "        filename (str): Name of the JSONL file to load\n",
        "        dataset_type (str): Type of dataset (e.g., \"training\" or \"evaluation\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (pandas DataFrame, Hugging Face Dataset)\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading {dataset_type} dataset...\")\n",
        "    df = load_jsonl_dataset(str(Path(artifact_dir) / filename))\n",
        "    df['text'] = df.apply(create_prompt, axis=1)\n",
        "    hf_dataset = Dataset.from_pandas(df)\n",
        "    print(f\"‚úÖ {dataset_type.capitalize()} dataset loaded with {len(df)} examples\")\n",
        "    return df, hf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9239842",
      "metadata": {
        "id": "f9239842"
      },
      "source": [
        "#### Let's prepare our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4602a18a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "4602a18a",
        "outputId": "ec723b7f-2c25-4b41-a2b4-3eafc276fdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Downloading dataset from Weights & Biases...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250515_210351-fbyttdve</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/fbyttdve' target=\"_blank\">fetch_astros_dataset</a></strong> to <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/fbyttdve' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/fbyttdve</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fetch_astros_dataset</strong> at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/fbyttdve' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/fbyttdve</a><br> View project at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250515_210351-fbyttdve/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset downloaded successfully!\n",
            "\n",
            "Loading training dataset...\n",
            "‚úÖ Successfully loaded dataset with no errors.\n",
            "‚úÖ Training dataset loaded with 1600 examples\n",
            "\n",
            "Dataset Statistics:\n",
            "Training examples: 1600\n",
            "\n",
            "Example prompt format:\n",
            "Question: What are 'Superluminous Supernovae' (SLSNe) and what distinguishes Type I SLSNe from normal Type Ia supernovae spectroscopically?\n",
            "Answer: Superluminous Supernovae (SLSNe) are much more luminous than normal Type Ia supernovae. Spectroscopically, Type I SLSNe are characterized by the absence of hydrogen and strong helium lines near peak light (like normal SNe Ia), but they show strong, broad metal lines, often including oxygen, magnesium, and calcium. Normal SNe Ia are defined by the presence of strong silicon absorption lines (Si II Œª6355) near peak light, which are often weak or absent in SLSNe I. The differences in spectra indicate different progenitor systems and explosion mechanisms: SNe Ia are thermonuclear disruptions of white dwarfs, while SLSNe I are thought to be core-collapse explosions of massive stars, often powered by magnetars or CSM interaction, despite their lack of hydrogen.</s>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Initialize W&B run and download dataset\n",
        "print(\"Step 1: Downloading dataset from Weights & Biases...\")\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                 project=WANDB_PROJECT_NAME,\n",
        "                 job_type=\"data_retrieval\",\n",
        "                 name=\"fetch_astros_dataset\"\n",
        "                 )\n",
        "\n",
        "# Download the dataset artifact\n",
        "artifact = run.use_artifact('wandb-registry-dataset/FC_FT_Workshop_Dataset:v4', type='dataset')\n",
        "dataset_dir = artifact.download()\n",
        "run.finish()\n",
        "print(\"‚úÖ Dataset downloaded successfully!\")\n",
        "\n",
        "# Step 2: Load and prepare datasets\n",
        "df_train, training_dataset = load_and_prepare_dataset(dataset_dir, \"astro_dataset_train.jsonl\", \"training\") #look at the helper fuctions if you're interested in how we prepare the data\n",
        "\n",
        "# Print dataset statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Training examples: {len(df_train)}\")\n",
        "print(\"\\nExample prompt format:\")\n",
        "print(df_train['text'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9045aafb",
      "metadata": {
        "id": "9045aafb"
      },
      "source": [
        "### üåå Dataset Loaded Successfully!\n",
        "\n",
        "At this point, we've:\n",
        "* Retrieved the Astros Dataset artifact\n",
        "* Loaded it into a pandas DataFrame\n",
        "* Created prompt-style text for fine-tuning\n",
        "* Converted it into a Hugging Face Dataset for training\n",
        "\n",
        "‚ú® Feel free to pause and explore the data before moving forward!\n",
        "\n",
        "Exploring the dataset can help you:\n",
        "\n",
        "* Understand the kinds of questions and answers the model will learn from\n",
        "* Check for any strange patterns, formatting issues, or interesting insights\n",
        "* Discover Easter Eggs\n",
        "\n",
        "üõ°Ô∏è We've added soft error handling while loading, so if you accidentally modify the dataset file, you'll be warned if any loading issues happen.\n",
        "\n",
        "üëâ Quick Tip: You don't need to modify the dataset to proceed, but if you want to explore, you can run things like:\n",
        "\n",
        "```\n",
        "print(df_train.sample(5))\n",
        "print(df_train['question'].apply(len).describe())\n",
        "print(df_train['answer'].apply(len).describe())\n",
        "```\n",
        "\n",
        "When you're ready, move on to loading the model and tokenizing the dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7001ed2e",
      "metadata": {
        "id": "7001ed2e"
      },
      "source": [
        "## üß† Model Vault: Load & Configure the Neural Core"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5caa9d0",
      "metadata": {
        "id": "e5caa9d0"
      },
      "source": [
        "## üöÄ Load Pretrained Model and Prepare Dataset for Fine-Tuning üå†\n",
        "In this section, we:\n",
        "\n",
        "* Retrieve the Model to Finetune from [W&B Registry FC_FT_Workshop_Model](https://wandb.ai/orgs/FullyConnected-2025-Workshops/registry/model?selectionPath=fullyconnected-2025-workshops%2Fwandb-registry-model%2FFC_FT_Workshop_Model&view=versions) TODO: Update for final version\n",
        "* Select and load a pretrained language model and its tokenizer from Hugging Face.\n",
        "* Format the Astros prompts into tokenized input IDs the model can understand.\n",
        "* Apply padding and truncation to keep sequence lengths manageable.\n",
        "* Split the tokenized dataset into training and validation sets (90% train / 10% validation).\n",
        "* Define a compute_metrics function to track TODO during fine-tuning.\n",
        "\n",
        "‚úÖ All the setup for model loading, tokenization, and data splitting is handled for you ‚Äî no manual steps required!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "627733f4",
      "metadata": {
        "id": "627733f4"
      },
      "source": [
        "#### Helper functions to Download, Load and Configure a Qunatized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eg99RRvdyl4-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "eg99RRvdyl4-",
        "outputId": "54e9b76f-6985-40cd-9833-6ba593f2f0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚¨áÔ∏è Downloading falcon-rw-1b (version v0) from Weights & Biases...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250515_210403-pro73kzq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/pro73kzq' target=\"_blank\">fetch_falcon-rw-1b_model</a></strong> to <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/pro73kzq' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/pro73kzq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact FC_FT_Workshop_Models:v0, 2505.10MB. 12 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   12 of 12 files downloaded.  \n",
            "Done. 0:0:15.7 (159.3MB/s)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fetch_falcon-rw-1b_model</strong> at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/pro73kzq' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/pro73kzq</a><br> View project at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250515_210403-pro73kzq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model saved to: models/falcon-rw-1b_v0\n",
            "‚úÖ Model downloaded successfully!\n",
            "\n",
            "‚¨áÔ∏è Downloading TinyLlama (version v1) from Weights & Biases...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250515_210432-dvnal1gl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/dvnal1gl' target=\"_blank\">fetch_TinyLlama_model</a></strong> to <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/dvnal1gl' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/dvnal1gl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact FC_FT_Workshop_Models:v1, 2100.44MB. 10 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   10 of 10 files downloaded.  \n",
            "Done. 0:0:13.6 (154.1MB/s)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fetch_TinyLlama_model</strong> at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/dvnal1gl' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/dvnal1gl</a><br> View project at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250515_210432-dvnal1gl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model saved to: models/TinyLlama_v1\n",
            "‚úÖ Model downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "#helper\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def download_all_models():\n",
        "    models = {\n",
        "        \"falcon-rw-1b\": \"v0\",\n",
        "        \"TinyLlama\": \"v1\",\n",
        "    }\n",
        "\n",
        "    for model_name, version in models.items():\n",
        "        print(f\"\\n‚¨áÔ∏è Downloading {model_name} (version {version}) from Weights & Biases...\")\n",
        "\n",
        "        run = wandb.init(\n",
        "            entity=WANDB_ENTITY,\n",
        "            project=WANDB_PROJECT_NAME,\n",
        "            job_type=\"model_retrieval\",\n",
        "            name=f\"fetch_{model_name}_model\"\n",
        "        )\n",
        "\n",
        "        artifact = run.use_artifact(\n",
        "            f'wandb-registry-model/FC_FT_Workshop_Models:{version}', type='model'\n",
        "        )\n",
        "        downloaded_dir = artifact.download()\n",
        "        run.finish()\n",
        "\n",
        "        local_path = Path(f\"./models/{model_name}_{version}\")\n",
        "\n",
        "        # Clear previous directory if exists\n",
        "        if local_path.exists():\n",
        "            shutil.rmtree(local_path)\n",
        "\n",
        "        shutil.move(downloaded_dir, local_path)\n",
        "\n",
        "        print(f\"‚úÖ Model saved to: {local_path}\")\n",
        "        print(f\"‚úÖ Model downloaded successfully!\")\n",
        "\n",
        "download_all_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9TqkI-UzqkR",
      "metadata": {
        "id": "Y9TqkI-UzqkR"
      },
      "outputs": [],
      "source": [
        "#helper\n",
        "def select_model():\n",
        "    \"\"\"\n",
        "    Display a menu of available models and let the user select one.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model_name, model_version)\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        \"1\": (\"falcon-rw-1b\", \"v0\"),\n",
        "        \"2\": (\"TinyLlama\", \"v1\"),  # Changed to v2 to match actual download\n",
        "    }\n",
        "\n",
        "    print(\"\\nAvailable Models:\")\n",
        "    print(\"1. Falcon RW 1B\")\n",
        "    print(\"2. TinyLlama 1B\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"\\nSelect a model (1-2): \").strip()\n",
        "        if choice in models:\n",
        "            model_name, version = models[choice]\n",
        "            print(f\"\\n‚úÖ Selected: {model_name}\")\n",
        "            return model_name, version\n",
        "        print(\"Invalid choice. Please select 1 or 2.\")\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"\n",
        "    Load the selected model and tokenizer from local cache with QLoRA configuration.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer, model_name)\n",
        "    \"\"\"\n",
        "    model_name, version = select_model()\n",
        "    model_dir = Path(f\"./models/{model_name}_{version}\")\n",
        "\n",
        "    print(f\"\\nüì¶ Loading model from: {model_dir}\")\n",
        "\n",
        "    # Step 1: Load tokenizer\n",
        "    print(\"\\nStep 1: Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úÖ Tokenizer loaded successfully!\")\n",
        "\n",
        "    # Step 2: Configure 4-bit quantization\n",
        "    print(\"\\nStep 2: Configuring QLoRA...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Step 3: Load model with quantization\n",
        "    print(\"Loading model with 4-bit quantization...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    if model_name == \"falcon-rw-1b\":\n",
        "        targets = [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n",
        "\n",
        "    if model_name == \"TinyLlama\":\n",
        "        targets = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
        "\n",
        "    # Step 4: Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=targets,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    print(\"‚úÖ Model loaded with QLoRA successfully!\")\n",
        "    %run {animate_dir}/celebrate_model.py\n",
        "    celebrate_model(model.base_model.model.__class__.__name__)\n",
        "    return model, tokenizer, model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sblwkf4QihQZ",
      "metadata": {
        "id": "Sblwkf4QihQZ"
      },
      "source": [
        "### Select Model\n",
        "You will be prompted to select one of the following models\n",
        "\n",
        "*   Option 1: [falcon-rw-1b](https://huggingface.co/tiiuae/falcon-rw-1b)\n",
        "*   Option 2: [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WLBfMTo6h5sr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "WLBfMTo6h5sr",
        "outputId": "f49cfa52-bd51-4448-a8d6-1682dd608afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available Models:\n",
            "1. Falcon RW 1B\n",
            "2. TinyLlama 1B\n",
            "\n",
            "Select a model (1-2): 1\n",
            "\n",
            "‚úÖ Selected: falcon-rw-1b\n",
            "\n",
            "üì¶ Loading model from: models/falcon-rw-1b_v0\n",
            "\n",
            "Step 1: Loading tokenizer...\n",
            "‚úÖ Tokenizer loaded successfully!\n",
            "\n",
            "Step 2: Configuring QLoRA...\n",
            "Loading model with 4-bit quantization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 12,582,912 || all params: 1,324,208,128 || trainable%: 0.9502\n",
            "‚úÖ Model loaded with QLoRA successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"celebration\" style=\"display: none; text-align: center; padding: 50px; font-family: 'Segoe UI', sans-serif;\">\n",
              "        <div style=\"\n",
              "            font-size: 20px;\n",
              "            color: #999;\n",
              "            margin-bottom: 10px;\n",
              "            letter-spacing: 1px;\n",
              "        \">\n",
              "            Mission Update\n",
              "        </div>\n",
              "        <div style=\"\n",
              "            font-size: 40px;\n",
              "            font-weight: bold;\n",
              "            color: #fac13c;\n",
              "            margin-bottom: 20px;\n",
              "        \">\n",
              "            Your model is ready for staging üöÄ\n",
              "        </div>\n",
              "        <div style=\"\n",
              "            font-size: 28px;\n",
              "            color: #ffffff;\n",
              "            background: #1a1a1a;\n",
              "            display: inline-block;\n",
              "            padding: 10px 20px;\n",
              "            border-radius: 10px;\n",
              "            box-shadow: 0 0 10px #fac13c;\n",
              "        \">\n",
              "            FalconForCausalLM\n",
              "        </div>\n",
              "    </div>\n",
              "\n",
              "    <script>\n",
              "    setTimeout(function() {\n",
              "        document.getElementById('celebration').style.display = 'block';\n",
              "    }, 1000);\n",
              "    </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model, tokenizer, model_name = load_model_and_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26597b6b",
      "metadata": {
        "id": "26597b6b"
      },
      "source": [
        "‚úÖ Model and tokenizer successfully loaded from the artifact!\n",
        "\n",
        "Next, we'll make a few adjustments to ensure the model handles padding correctly,\n",
        "and then prepare our dataset for training by tokenizing the input prompts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dff0e01",
      "metadata": {
        "id": "1dff0e01"
      },
      "source": [
        "## üîÑ Tokenize & Split: Format Data for Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd1d48c",
      "metadata": {
        "id": "0bd1d48c"
      },
      "source": [
        "#### Helper fuctions for tokenizing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d0f0ac",
      "metadata": {
        "id": "29d0f0ac"
      },
      "outputs": [],
      "source": [
        "#helper\n",
        "# Set pad token and pad token ID if missing (important for consistent model behavior)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Function to tokenize input prompts for training\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize the text prompts with padding and truncation.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A batch of examples with 'text' field.\n",
        "\n",
        "    Returns:\n",
        "        dict: Tokenized outputs (input_ids, attention_mask, etc.)\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Will be adding a few metric functions users can use to test out out the model - TODO: Cleanup , either remove or add addititions functions - not currently used\n",
        "def compute_perpexity(eval_preds):\n",
        "    \"\"\"\n",
        "    Compute perplexity metric from model evaluation logits.\n",
        "\n",
        "    Args:\n",
        "        eval_preds (tuple): Tuple of (logits, labels) from evaluation step.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing 'perplexity' score.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_preds\n",
        "    if not isinstance(logits, torch.Tensor):\n",
        "        logits = torch.tensor(logits)\n",
        "    if not isinstance(labels, torch.Tensor):\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # Shift logits and labels for causal language modeling\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = labels[:, 1:].contiguous()\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    loss = loss_fct(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1)\n",
        "    )\n",
        "    perplexity = math.exp(loss.item()) if loss.item() < 100 else float(\"inf\")\n",
        "    return {\"perplexity\": perplexity}\n",
        "\n",
        "def tokenized_train_test(training_dataset, split):\n",
        "  # Apply the tokenizer to the datasets\n",
        "    split_dataset = training_dataset.train_test_split(test_size=split)\n",
        "    train_dataset = split_dataset[\"train\"]\n",
        "    eval_dataset = split_dataset[\"test\"]\n",
        "    train_dataset = train_dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=train_dataset.column_names,\n",
        "    )\n",
        "\n",
        "    eval_dataset = eval_dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=eval_dataset.column_names,\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Tokenization applied to Training & Evaluation Datasets successfully!\")\n",
        "\n",
        "    return train_dataset, eval_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6780204",
      "metadata": {
        "id": "c6780204"
      },
      "source": [
        "#### Load the datasets\n",
        "\n",
        "You can modify how our training data is passed to our training script to finetune the model. Make sure to analyze the data so you can select an appropriate **Sample Size** and  **Train/Test split** for the finetuning process.\n",
        "\n",
        "TO-DO: Update the values below for your first run. You can come back and try it with new values again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61faf47c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "6c83dd832f784383aa67a7bac1c81722",
            "02b048fcb50c451d89964ee93405cb93",
            "f7d0f2a7a7df4589b2ff1ea0562b8ae5",
            "ce5267e4265c49b08e066b4a5806e8cf",
            "c7a0ffa43bfd45bb82a110520ef065c6",
            "673061059cb64053896079a0de7d3458",
            "908c81b282f64785b4fa729646b00c98",
            "1173b7a21b3a4792bda7baa084fc5147",
            "b45860c163b142189b6eb2eaacdcc1f4",
            "9f3f09b8e4bc411880e0d389c2043a88",
            "da44f8d632714e7d89b2222686eafa5f",
            "c6c6c1949f1b4bbc9624647c4640dff9",
            "52c005c417a84f9aa9604239dd62197e",
            "f5926d20d7dc4bc5a784126f57867a59",
            "49be6b548baf43c5bcd1091e2bfd56f6",
            "ebfc99985a16470e8e49c9b788c18302",
            "3b119211b51a4c1e847bf9c8e2fbd28c",
            "ad5f99fdfdd942fab037f03f0f6c9107",
            "6fc3ad2df4a64a2aaba0d4567e3a9277",
            "ef60343739754ac7a5d61a82749f2f30",
            "e1ec82f4ca08434b85d3329bad4bf301",
            "ed9744b33f834485b653da828cf72026"
          ]
        },
        "id": "61faf47c",
        "outputId": "a6bde064-b7fb-4d79-c860-14a63d123c99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c83dd832f784383aa67a7bac1c81722",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6c6c1949f1b4bbc9624647c4640dff9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tokenization applied to Training & Evaluation Datasets successfully!\n"
          ]
        }
      ],
      "source": [
        "training_sample = training_dataset.shuffle(seed=42).select(range(1000)) # chosse between 100 and 1600 samples\n",
        "\n",
        "train_test_split = 0.1 # choose a float value between 0 and 1\n",
        "\n",
        "train_dataset, eval_dataset = tokenized_train_test(training_dataset, train_test_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08b86f5",
      "metadata": {
        "id": "e08b86f5"
      },
      "source": [
        "## ‚öôÔ∏è Training Command Center\n",
        "Set training arguments to guide your model's learning trajectory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e65c01f",
      "metadata": {
        "id": "5e65c01f"
      },
      "source": [
        "## üõ∞Ô∏è Training Arguments (Where You Fine-Tune Settings) üåô\n",
        "\n",
        "This is where you'll do most of your experimentation! üéØ\n",
        "\n",
        "The `TrainingArguments` object controls how your model is fine-tuned, including:\n",
        "\n",
        "* Batch size\n",
        "* Number of epochs\n",
        "* Learning rate\n",
        "* Warmup steps\n",
        "* Mixed precision (fp16) for faster training\n",
        "* Checkpoint saving\n",
        "* Reporting to Weights & Biases\n",
        "\n",
        "You can modify the hyperparameters here to see how different settings impact model performance.\n",
        "\n",
        "üî• Pro Tip: TODO INSERT SOME TIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013c0d65",
      "metadata": {
        "id": "013c0d65"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # run_name=f\"fine-tuning-{model.__class__.__name__}-qlora\",\n",
        "    run_name=f\"fine-tuning-{model_name}-qlora\",\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=10,\n",
        "    #The following is commented out for now, used by me for optimization runs\n",
        "    # per_device_train_batch_size=64,\n",
        "    # per_device_eval_batch_size=32,\n",
        "    # dataloader_num_workers=4,\n",
        "    # gradient_accumulation_steps=16,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=4,\n",
        "    dataloader_num_workers=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    gradient_checkpointing=False, #Choose to store the full forward-pass activations in GPU RAM\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    group_by_length=True,\n",
        "    report_to=[\"wandb\"],\n",
        "    remove_unused_columns=True,\n",
        "    dataloader_pin_memory=True,\n",
        "    optim=\"adamw_torch\", #See https://huggingface.co/docs/transformers/v4.51.3/en/perf_train_gpu_one#optimizers\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\", # See https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.SchedulerType\n",
        "    auto_find_batch_size=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=5,\n",
        "    load_best_model_at_end=True,\n",
        "    # metric_for_best_model=\"perplexity\",\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    logging_strategy=\"steps\",\n",
        "    label_names=[\"labels\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9dcc2dd",
      "metadata": {
        "id": "a9dcc2dd"
      },
      "source": [
        "## üõ∞Ô∏è Engage Training Tracker\n",
        "Launch the model and track training live with W&B."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7faff1ee",
      "metadata": {
        "id": "7faff1ee"
      },
      "source": [
        "## üî≠ Initialize Trainer, Train, and Save üåé\n",
        "\n",
        "In this final section:\n",
        "\n",
        "* We initialize the Trainer with:\n",
        "  * The model\n",
        "  * The datasets (train/test splits)\n",
        "  * The training arguments\n",
        "  * A data collator for language modeling\n",
        "  * Our compute_metrics function to calculate TODO\n",
        "\n",
        "* We start training by calling trainer.train().\n",
        "* We save the fine-tuned model and tokenizer locally.\n",
        "* We finish the W&B run to close the logging cleanly.\n",
        "\n",
        "üß† Reminder: After training finishes, your fine-tuned model will be available in your local runtime ‚Äî you can upload it back to W&B or Hugging Face later!\n",
        "\n",
        "üö® Training Ahead: Be ready for 10-15 min runtimes with the default configs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CiGIHshY1c0m",
      "metadata": {
        "id": "CiGIHshY1c0m"
      },
      "source": [
        "Running into the following?\n",
        "\n",
        "> AttributeError: `AcceleratorState` object has no attribute `distributed_type`\n",
        "\n",
        "This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.\n",
        "\n",
        "To Fix Run the following in a new cell before reinitalizing your trainer\n",
        "\n",
        "```\n",
        "from accelerate.state import AcceleratorState\n",
        "AcceleratorState._reset_state()\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w1DD36UhE0QJ",
      "metadata": {
        "id": "w1DD36UhE0QJ"
      },
      "outputs": [],
      "source": [
        "# from accelerate.state import AcceleratorState\n",
        "# AcceleratorState._reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050c4bfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "050c4bfd",
        "outputId": "eba166d0-e6fd-4e57-a776-0d578eb94695"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"launch-container\" style=\"position: relative; width: 100%; height: 600px; background: radial-gradient(ellipse at bottom, #000 0%, #020111 100%); overflow: hidden; font-family: 'Comic Sans MS', cursive, sans-serif;\">\n",
              "\n",
              "<style>\n",
              "  /* --- Same CSS as before --- */\n",
              "  #launch-container .stars {\n",
              "    position: absolute;\n",
              "    width: 100%;\n",
              "    height: 100%;\n",
              "    overflow: hidden;\n",
              "    z-index: 1;\n",
              "  }\n",
              "  #launch-container .star {\n",
              "    position: absolute;\n",
              "    width: 2px;\n",
              "    height: 2px;\n",
              "    background: white;\n",
              "    border-radius: 50%;\n",
              "    opacity: 0.7;\n",
              "    animation: moveStar 5s linear infinite;\n",
              "  }\n",
              "  @keyframes moveStar {\n",
              "    0% { transform: translateY(0); }\n",
              "    100% { transform: translateY(600px); }\n",
              "  }\n",
              "  #launch-container .countdown {\n",
              "    position: absolute;\n",
              "    width: 100%;\n",
              "    top: 30%;\n",
              "    font-size: 4em;\n",
              "    text-align: center;\n",
              "    z-index: 2;\n",
              "    color: #fff;\n",
              "    text-shadow: 0 0 10px #0ff;\n",
              "  }\n",
              "  #launch-container .rocket {\n",
              "      position: absolute; /* <-- must stay absolute */\n",
              "      bottom: 20px;\n",
              "      left: 50%;\n",
              "      transform: translateX(-50%);\n",
              "      font-size: 80px;\n",
              "      z-index: 3;\n",
              "      transition: transform 3s ease-out, bottom 3s ease-out;\n",
              "  }\n",
              "  #launch-container .flame {\n",
              "      position: absolute; /* <-- stays absolute INSIDE rocket */\n",
              "      top: 100%; /* start exactly under the rocket */\n",
              "      left: 50%;\n",
              "      transform: translateX(-50%) translateY(0);\n",
              "      width: 20px;\n",
              "      height: 40px;\n",
              "      background: radial-gradient(circle, orange 0%, red 100%);\n",
              "      border-radius: 50%;\n",
              "      animation: flameFlicker 0.2s infinite alternate;\n",
              "      opacity: 0;\n",
              "      z-index: 1;\n",
              "  }\n",
              "  @keyframes flameFlicker {\n",
              "    0% { transform: translateX(-50%) scaleY(2); }\n",
              "    100% { transform: translateX(-50%) scaleY(1.5); }\n",
              "  }\n",
              "  #launch-container #control-panel {\n",
              "    display: none;\n",
              "    position: absolute;\n",
              "    top: 30%;\n",
              "    left: 50%;\n",
              "    transform: translateX(-50%);\n",
              "    background: rgba(0,0,0,0.8);\n",
              "    padding: 20px;\n",
              "    border: 2px solid #0ff;\n",
              "    border-radius: 10px;\n",
              "    width: 300px;\n",
              "    z-index: 5;\n",
              "    text-align: center;\n",
              "  }\n",
              "  #launch-container button {\n",
              "    margin-top: 10px;\n",
              "    background: #03A9F4;\n",
              "    border: none;\n",
              "    color: white;\n",
              "    padding: 10px 20px;\n",
              "    font-size: 1em;\n",
              "    border-radius: 5px;\n",
              "    cursor: pointer;\n",
              "  }\n",
              "  #launch-container .confetti {\n",
              "    position: absolute;\n",
              "    width: 8px;\n",
              "    height: 8px;\n",
              "    background: hsl(var(--hue), 70%, 60%);\n",
              "    animation: fall 3s ease-out forwards;\n",
              "    z-index: 4;\n",
              "  }\n",
              "  @keyframes fall {\n",
              "    to {\n",
              "      transform: translateY(600px) rotate(720deg);\n",
              "      opacity: 0;\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "<div class=\"stars\" id=\"stars\"></div>\n",
              "<div class=\"countdown\" id=\"countdown\">Preparing...</div>\n",
              "<div class=\"rocket\" id=\"rocket\">\n",
              "    üõ∏\n",
              "    <div class=\"flame\" id=\"flame\"></div>\n",
              "</div>\n",
              "\n",
              "\n",
              "<div id=\"control-panel\">\n",
              "  <h3>üöÄ Fine-Tuning Mission Control</h3>\n",
              "  <div style='text-align: left; margin-bottom: 4px;'><strong>run_name:</strong> fine-tuning-falcon-rw-1b-qlora</div><div style='text-align: left; margin-bottom: 4px;'><strong>num_train_epochs:</strong> 10</div><div style='text-align: left; margin-bottom: 4px;'><strong>per_device_train_batch_size:</strong> 16</div><div style='text-align: left; margin-bottom: 4px;'><strong>learning_rate:</strong> 0.0002</div><div style='text-align: left; margin-bottom: 4px;'><strong>optim:</strong> OptimizerNames.ADAMW_TORCH</div><div style='text-align: left; margin-bottom: 4px;'><strong>fp16:</strong> ‚úÖ</div><div style='text-align: left; margin-bottom: 4px;'><strong>gradient_checkpointing:</strong> ‚ùå</div><div style='text-align: left; margin-bottom: 4px;'><strong>eval_strategy:</strong> IntervalStrategy.EPOCH</div><div style='text-align: left; margin-bottom: 4px;'><strong>save_strategy:</strong> SaveStrategy.EPOCH</div>\n",
              "  <button id=\"ignite-btn\" onclick=\"ignite()\">Ignite</button>\n",
              "</div>\n",
              "\n",
              "<script>\n",
              "// --- Ignite Button Handler ---\n",
              "function ignite() {\n",
              "  console.log('üöÄ Ignite button clicked...');\n",
              "  const btn = document.getElementById('ignite-btn');\n",
              "  btn.disabled = true;\n",
              "  btn.innerText = 'Fine-tuning... ‚è≥';\n",
              "  const code = 'ignite_training()';\n",
              "\n",
              "  // Classic IPython Notebook\n",
              "  if (window.IPython && IPython.notebook && IPython.notebook.kernel) {\n",
              "    IPython.notebook.kernel.execute(code);\n",
              "\n",
              "  // Classic Jupyter global (some older labs)\n",
              "  } else if (window.Jupyter && Jupyter.notebook && Jupyter.notebook.kernel) {\n",
              "    Jupyter.notebook.kernel.execute(code);\n",
              "\n",
              "  // Fallback via require ‚Äî often works in JupyterLab or VS Code\n",
              "  } else if (typeof require === 'function') {\n",
              "    require(['base/js/namespace'], function(Jupyter){\n",
              "      if (Jupyter.notebook && Jupyter.notebook.kernel) {\n",
              "        Jupyter.notebook.kernel.execute(code);\n",
              "      } else {\n",
              "        console.error('üöÄ Ignite: kernel not found in require callback');\n",
              "      }\n",
              "    });\n",
              "\n",
              "  // Give up\n",
              "  } else {\n",
              "    console.error('üöÄ Cannot ignite: no Notebook kernel API detected');\n",
              "    alert('üöÄ Cannot ignite: not running inside a supported Jupyter environment.');\n",
              "  }\n",
              "}\n",
              "\n",
              "// --- Main Launch Animation Setup ---\n",
              "(function() {\n",
              "  const container = document.getElementById('launch-container');\n",
              "  const stars = document.getElementById('stars');\n",
              "  const countdownEl = document.getElementById('countdown');\n",
              "  const rocket = document.getElementById('rocket');\n",
              "  const flame = document.getElementById('flame');\n",
              "  const panel = document.getElementById('control-panel');\n",
              "\n",
              "  // Create stars\n",
              "  for (let i = 0; i < 150; i++) {\n",
              "    let star = document.createElement('div');\n",
              "    star.className = 'star';\n",
              "    star.style.top = Math.random() * 100 + '%';\n",
              "    star.style.left = Math.random() * 100 + '%';\n",
              "    star.style.animationDuration = (2 + Math.random() * 3) + 's';\n",
              "    stars.appendChild(star);\n",
              "  }\n",
              "\n",
              "  // Countdown and launch\n",
              "  let countdown = 3;\n",
              "  countdownEl.innerText = countdown;\n",
              "  const interval = setInterval(() => {\n",
              "    countdown--;\n",
              "    if (countdown > 0) {\n",
              "      countdownEl.innerText = countdown;\n",
              "    } else if (countdown === 0) {\n",
              "      countdownEl.innerText = \"Liftoff!\";\n",
              "      rocket.style.bottom = '600px';\n",
              "      rocket.style.transform = 'translateX(-50%) translateY(-200px)';\n",
              "      flame.style.opacity = 1;\n",
              "      setTimeout(() => {\n",
              "        rocket.style.display = 'none';\n",
              "        flame.style.display = 'none';\n",
              "        countdownEl.style.display = 'none';\n",
              "        panel.style.display = 'block';\n",
              "        releaseConfetti();\n",
              "      }, 3000);\n",
              "      clearInterval(interval);\n",
              "    }\n",
              "  }, 1000);\n",
              "\n",
              "  // Confetti\n",
              "  function releaseConfetti() {\n",
              "    for (let i = 0; i < 50; i++) {\n",
              "      let c = document.createElement('div');\n",
              "      c.className = 'confetti';\n",
              "      c.style.left = Math.random() * 100 + '%';\n",
              "      c.style.top = '-10px';\n",
              "      c.style.setProperty('--hue', Math.random() * 360);\n",
              "      container.appendChild(c);\n",
              "      setTimeout(() => c.remove(), 3000);\n",
              "    }\n",
              "  }\n",
              "})();\n",
              "</script>\n",
              "\n",
              "\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Configure model for training\n",
        "model.config.use_cache = False  # Disable cache during training\n",
        "\n",
        "# Set label names for PEFT model\n",
        "model.config.label_names = [\"labels\"]\n",
        "\n",
        "# Initialize trainer with modified configuration\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "        pad_to_multiple_of=8  # Add padding to multiple of 8 for better performance\n",
        "    ),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    #compute_metrics=compute_perpexity, #If monitoring additional metric(s) can set this argument accordingly to your compute functions\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing with the new format\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "#Working Idea - Launch training via animation - #TODO Figure out how to pipe the ignite button or just write simple message to run next cell to train model\n",
        "def ignite_training():\n",
        "    # This function will be called by the \"Ignite\" button\n",
        "    print(\"üöÄ Starting training process...\")\n",
        "\n",
        "    # Your existing training code\n",
        "    run = wandb.init(entity=WANDB_ENTITY,\n",
        "                    project=WANDB_PROJECT_NAME,\n",
        "                    job_type=\"finetuning_job\",\n",
        "                    # name=f\"fine_tune_{model.__class__.__name__}\"\n",
        "                     name = f\"fine_tune_{model_name}\"\n",
        "                    )\n",
        "    train_output = trainer.train()\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    print(\"‚úÖ Training complete!\")\n",
        "    return train_output\n",
        "\n",
        "%run {animate_dir}/launch_sequence.py\n",
        "launch_sequence(training_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7bc5c7d",
      "metadata": {
        "id": "f7bc5c7d"
      },
      "source": [
        "## ‚öô Now we kick off the training process ‚öô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93378c7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "93378c7e",
        "outputId": "85122374-7278-468f-82f2-958246345a2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250514_051351-wo7bs7xh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/wo7bs7xh' target=\"_blank\">fine_tune_falcon-rw-1b</a></strong> to <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/wo7bs7xh' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/wo7bs7xh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='161' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [161/220 13:58 < 05:11, 0.19 it/s, Epoch 7/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.366200</td>\n",
              "      <td>2.232309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.106800</td>\n",
              "      <td>2.094196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.918700</td>\n",
              "      <td>2.041408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.746700</td>\n",
              "      <td>2.030147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.581700</td>\n",
              "      <td>2.047748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.423400</td>\n",
              "      <td>2.081184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.316800</td>\n",
              "      <td>2.121665</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ</td></tr><tr><td>eval/runtime</td><td>‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÇ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñá‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñá‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.12166</td></tr><tr><td>eval/runtime</td><td>4.8977</td></tr><tr><td>eval/samples_per_second</td><td>32.668</td></tr><tr><td>eval/steps_per_second</td><td>8.167</td></tr><tr><td>total_flos</td><td>3.781493822324736e+16</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>161</td></tr><tr><td>train/grad_norm</td><td>1.39991</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>1.3168</td></tr><tr><td>train_loss</td><td>1.75819</td></tr><tr><td>train_runtime</td><td>843.7561</td></tr><tr><td>train_samples_per_second</td><td>17.067</td></tr><tr><td>train_steps_per_second</td><td>0.261</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine_tune_falcon-rw-1b</strong> at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/wo7bs7xh' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/wo7bs7xh</a><br> View project at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250514_051351-wo7bs7xh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Train\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                  project=WANDB_PROJECT_NAME,\n",
        "                  job_type=\"finetuning_job\",\n",
        "                  # name=f\"fine_tune_{model.__class__.__name__}\"\n",
        "                  name = f\"fine_tune_{model_name}\"\n",
        "                  )\n",
        "train_output = trainer.train()\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "# launch_sequence(training_args) #TODO Test this in Sagemaker - Otherwise Remove Ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f4c42f",
      "metadata": {
        "id": "00f4c42f"
      },
      "source": [
        "## üíæ Save & Upload\n",
        "Preserve your fine-tuned model as a W&B artifact."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8e32a1",
      "metadata": {
        "id": "6b8e32a1"
      },
      "source": [
        "We will now save this model to W&B. You will need the artifact refernce for this model to get points for completing the fine tuning section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5891f1",
      "metadata": {
        "id": "1c5891f1"
      },
      "source": [
        "Tracking your model in W&B can be really helpful:\n",
        "\n",
        "- You can now share this model with your team and beyond\n",
        "- W&B creates a lineage map of your model so you can see the full model lifecycle dataset->training->final state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f428cdbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "f428cdbc",
        "outputId": "4798665a-7613-4a82-eed3-2b67ba057f96"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FT-Best-Model-Upload</strong> at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/s3d7w0a5' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/s3d7w0a5</a><br> View project at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a><br>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250514_050112-s3d7w0a5/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250514_050134-02a8odzf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/02a8odzf' target=\"_blank\">FT-Best-Model-Upload</a></strong> to <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/02a8odzf' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/02a8odzf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./best_model)... Done. 0.1s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FT-Best-Model-Upload</strong> at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/02a8odzf' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop/runs/02a8odzf</a><br> View project at: <a href='https://wandb.ai/FT-Testing/Astros-FT-Workshop' target=\"_blank\">https://wandb.ai/FT-Testing/Astros-FT-Workshop</a><br>Synced 5 W&B file(s), 0 media file(s), 9 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250514_050134-02a8odzf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Saving and uploading best model - working draft\n",
        "trainer.save_model(f\"./best_model/{type(model.base_model.model).__name__}\")\n",
        "tokenizer.save_pretrained(f\"./best_model/{type(model.base_model.model).__name__}\")\n",
        "\n",
        "run = wandb.init(project=WANDB_PROJECT_NAME,\n",
        "                 entity=WANDB_ENTITY,\n",
        "                 job_type=\"FT-Workshop-Finetuning-Best-Model-Upload\",\n",
        "                 name=\"FT-Best-Model-Upload\")\n",
        "\n",
        "artifact = wandb.Artifact(\n",
        "    name=f\"{WORKSHOP_TEAM_NAME}-ft-best-model-{type(model.base_model.model).__name__}\",\n",
        "    type=\"model\",\n",
        "    description=\"\"\"Best FineTuned model from the Astros-FT-Workshop.\"\"\"\n",
        ")\n",
        "\n",
        "artifact.add_dir(local_path=\"./best_model\")\n",
        "\n",
        "logged_artifact = run.log_artifact(artifact)\n",
        "\n",
        "run.link_artifact(\n",
        "  artifact=logged_artifact,\n",
        "  target_path=\"wandb-registry-model/FC_FT_Workshop_FineTuned_Models\"\n",
        ")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e90dd70",
      "metadata": {
        "id": "2e90dd70"
      },
      "source": [
        "## ‚úÖ Mission Checkpoint: Model Finetuned\n",
        "\n",
        "Congratulations, Architect! You've:\n",
        "- Loaded and prepped your training dataset ‚úÖ\n",
        "- Configured a foundational model ‚úÖ\n",
        "- Finetuned it with parameter-efficient methods ‚úÖ\n",
        "- Logged your training runs and saved the final model to Weights & Biases ‚úÖ\n",
        "\n",
        "Your model is now part of your mission's neural infrastructure.\n",
        "\n",
        "Next, we prepare to test and evaluate. But first, a quick system check..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e953f25",
      "metadata": {
        "id": "8e953f25"
      },
      "source": [
        "## üß∞ Systems Maintenance Bay: Utilities\n",
        "\n",
        "Before testing, it's wise to flush memory and check your hardware status. Use these utilities to prepare the environment.\n",
        "\n",
        "Just like a good engineer, make sure the ship's neural bays are cleared and ready."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b761f2d7",
      "metadata": {
        "id": "b761f2d7"
      },
      "source": [
        "## Utilities üß∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233f922d",
      "metadata": {
        "id": "233f922d"
      },
      "outputs": [],
      "source": [
        "# -- Flush out GPU memory - when required - may require restarting the notebook\n",
        "import gc, torch\n",
        "\n",
        "try:\n",
        "    del trainer\n",
        "except: print(\"cannot release memory\")\n",
        "try:\n",
        "    del model\n",
        "except: print(\"cannot release memory\")\n",
        "try:\n",
        "    del tokenizer\n",
        "except: print(\"cannot release memory\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a767a0d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a767a0d9",
        "outputId": "8e81b801-5015-4fbd-a287-5e0966f7e6f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 14 03:37:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0             32W /   70W |    8940MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0f71a7",
      "metadata": {
        "id": "5d0f71a7"
      },
      "source": [
        "## üß™ Testing the Neural Core\n",
        "\n",
        "Now that your model is trained and uploaded, it‚Äôs time to test your ship‚Äôs new neural core.\n",
        "\n",
        "You‚Äôll load the fine-tuned model and run test prompts to ensure it responds with precision and depth‚Äîcritical for deep-space operations.\n",
        "\n",
        "We‚Äôve equipped you with a call function wrapped in `Weave`, our GenAI interface and telemetry layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf60a175",
      "metadata": {
        "id": "bf60a175"
      },
      "source": [
        "# üîß Testing our model ü™õ\n",
        "\n",
        "Let's start by creating some helper functions to load and call the model we just trained."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eefdfa6c",
      "metadata": {
        "id": "eefdfa6c"
      },
      "source": [
        "Since we created an adapter during the finetuning process, our load model function loads the original model along with our adapter using PEFT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e8abc80",
      "metadata": {
        "id": "0e8abc80"
      },
      "source": [
        "## üõ∞Ô∏è Introducing Weave: Your AI Telemetry and Evaluation Suite\n",
        "\n",
        "**Weave** is Weights & Biases‚Äô next-gen platform for tracking, evaluating, and visualizing GenAI applications.\n",
        "\n",
        "In REBOOT, you'll use Weave to:\n",
        "- Log and score model generations\n",
        "- Run structured evaluations on Q&A performance\n",
        "- Compare outputs with reference answers\n",
        "\n",
        "This enables you to **quantitatively assess** how mission-ready your model is.\n",
        "\n",
        "Let‚Äôs initialize Weave and plug it into your finetuned system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7348cba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7348cba",
        "outputId": "8e986fcd-5e33-44ea-cee8-f34ede1d011c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/503.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m501.8/503.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install weave \"weave[scorers]\" -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309fd38a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "309fd38a",
        "outputId": "4155a61d-a420-4e3a-e7b4-ce506a9ed1f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as Weights & Biases user: mohammadbakir.\n",
            "View Weave data at https://wandb.ai/FT-Testing/Astros-FT-Workshop/weave\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<weave.trace.weave_client.WeaveClient at 0x7a163e1c1950>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import weave\n",
        "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f2de66",
      "metadata": {
        "id": "57f2de66"
      },
      "source": [
        "#### Helper functions to load our local model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1572e137",
      "metadata": {
        "id": "1572e137"
      },
      "outputs": [],
      "source": [
        "#helper\n",
        "def load_finetuned_model(adapter_dir, base_model_dir):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\n",
        "\n",
        "    # Load base model\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load model with quantization\n",
        "    print(\"Loading model with 4-bit quantization...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        Path(base_model_dir),\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        )\n",
        "    # Load fine-tuned adapter\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F4vH2PsqWsDD",
      "metadata": {
        "id": "F4vH2PsqWsDD"
      },
      "source": [
        "### Calling our Local Finetuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473bb674",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "473bb674",
        "outputId": "c4eb4471-34d1-4095-f776-242bdb4231ab"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'weave' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ff9eab6e5a35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mweave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"Generate an answer from your Local LLM given a prompt.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msystem_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"You are an expert in astrophysics. Please provide a concise and truthful answer to the following question:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'weave' is not defined"
          ]
        }
      ],
      "source": [
        "@weave.op()\n",
        "def call_model(question: str) -> str:\n",
        "    \"\"\"Generate an answer from your Local LLM given a prompt.\"\"\"\n",
        "\n",
        "    system_prompt = \"You are an expert in astrophysics. Please provide a concise and truthful answer to the following question:\"\n",
        "    prompt = system_prompt + \"\\n\\n\" + question + \"\\nAnswer:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=150, do_sample=False, eos_token_id=tokenizer.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).replace(prompt, '').strip(),"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd25f02a",
      "metadata": {
        "id": "bd25f02a"
      },
      "source": [
        "## üìä Final Check: Evaluation Protocols\n",
        "\n",
        "Your neural core is active‚Äîbut is it mission-grade?\n",
        "\n",
        "Use this section to:\n",
        "- Load an evaluation dataset\n",
        "- Score model responses using embedding similarity\n",
        "- Track performance with W&B + Weave\n",
        "\n",
        "**Evaluation is critical** before deployment‚Äîit ensures your model‚Äôs reasoning is aligned with mission parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b39f03",
      "metadata": {
        "id": "a4b39f03"
      },
      "source": [
        "# Evaluating our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "700a6f0f",
      "metadata": {
        "id": "700a6f0f"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf82b834",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf82b834",
        "outputId": "5119b1d1-e3b7-47c2-ea1a-21105b2a7ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with 4-bit quantization...\n"
          ]
        }
      ],
      "source": [
        "base_model_dir = \"./models/TinyLlama_v1\" # Path to base model - modify accordingly to fine_tuned_model/<TinyLlama_v1 or falcon-rw-1b_v0>\n",
        "adapter_dir = f\"./best_model/{type(model.base_model.model).__name__}\" #add path to adapter dir\n",
        "\n",
        "tokenizer, model = load_finetuned_model(adapter_dir, base_model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53cf9ddb",
      "metadata": {
        "id": "53cf9ddb"
      },
      "source": [
        "## Get Eval Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db236752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db236752",
        "outputId": "2123d874-4710-4544-c9bf-aa2909dab1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as Weights & Biases user: mohammadbakir.\n",
            "View Weave data at https://wandb.ai/fc25-wandb-admins/uncategorized/weave\n"
          ]
        }
      ],
      "source": [
        "weave.init('fc25-wandb-admins/uncategorized')\n",
        "eval_dataset_public_v0 = weave.ref('eval_dataset_public:v0').get()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d915b61",
      "metadata": {
        "id": "2d915b61"
      },
      "source": [
        "## Test the model with a sample from our eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f64721",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79f64721",
        "outputId": "71aabdb8-48ee-4498-da9e-453bacd5dbd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/fc25-wandb-admins/uncategorized/r/call/0196cd2e-7000-7f81-89ac-6757fbec2ea2\n",
            "üõ∞Ô∏è  Incoming Transmission ‚Äî Mission Q&A\n",
            "\n",
            "üß† Question:\n",
            "How can 'cosmic variance' affect the interpretation of measurements of the cosmic star formation rate density?\n",
            "\n",
            "ü§ñ Model Response:\n",
            "Cosmic variance is the uncertainty in measuring a quantity (like the number of galaxies or stars) due to the fact that the number of galaxies or stars observed is not independent of the number of galaxies or stars that were ever formed. This means that the observed number of galaxies or stars is a random variable, and the observed distribution of galaxies or stars is not a true probability distribution. This means that the observed cosmic star formation rate density (SFRD) is not a true measure of the total amount of star formation in the universe. It is a random variable, and its true value depends on the initial conditions (like the initial mass function) and the initial conditions of the universe. It is therefore not a reliable measure of the total amount\n"
          ]
        }
      ],
      "source": [
        "question = eval_dataset_public_v0[20]['question']\n",
        "answer = call_model(question)\n",
        "\n",
        "print(\"üõ∞Ô∏è  Incoming Transmission ‚Äî Mission Q&A\\n\")\n",
        "print(f\"üß† Question:\\n{question}\\n\")\n",
        "print(f\"ü§ñ Model Response:\\n{answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf1d927",
      "metadata": {
        "id": "5bf1d927"
      },
      "source": [
        "## Setup eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0cfaee",
      "metadata": {
        "id": "ff0cfaee"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from weave.scorers import EmbeddingSimilarityScorer\n",
        "similarity_scorer = EmbeddingSimilarityScorer(\n",
        "    model_id=\"openai/text-embedding-3-small\",  # will need to update this to bedrock's titan models\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "similarity_scorer.column_map = {\n",
        "    \"output\": \"model_output\",  # Your model's response\n",
        "    \"target\": \"answer\", # The reference response (expected)\n",
        "    \"kwargs\": \"question\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5479015",
      "metadata": {
        "id": "a5479015"
      },
      "outputs": [],
      "source": [
        "eval_dataset = eval_dataset_public_v0.rows[20:25] # select samples to run evalaution against"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533ef801",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "533ef801",
        "outputId": "55475355-aecc-4822-c3fc-9a085ff202e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: trace.wandb.ai. Connection pool size: 10\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: trace.wandb.ai. Connection pool size: 10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/fc25-wandb-admins/uncategorized/r/call/0196cd2e-a20c-76c3-9ce1-903945e45e40\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'EmbeddingSimilarityScorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'similarity_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8166838007224524</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_similar'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72.45420384407043</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'EmbeddingSimilarityScorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'similarity_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.8166838007224524\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[32m'is_similar'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m72.45420384407043\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'EmbeddingSimilarityScorer': {'similarity_score': {'mean': 0.8166838007224524}, 'is_similar': {'true_count': 5, 'true_fraction': 1.0}}, 'model_latency': {'mean': 72.45420384407043}}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')# will remove this once we move to bedrock\n",
        "\n",
        "evaluation = weave.Evaluation(\n",
        "    evaluation_name = f\"{WORKSHOP_TEAM_NAME}_qna_eval\",\n",
        "    dataset=eval_dataset, scorers=[similarity_scorer],\n",
        "    name=\"model_qna_eval\"\n",
        "   )\n",
        "\n",
        "print(await(evaluation.evaluate(call_model)) )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02b048fcb50c451d89964ee93405cb93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_673061059cb64053896079a0de7d3458",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_908c81b282f64785b4fa729646b00c98",
            "value": "Map:‚Äá100%"
          }
        },
        "1173b7a21b3a4792bda7baa084fc5147": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b119211b51a4c1e847bf9c8e2fbd28c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49be6b548baf43c5bcd1091e2bfd56f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ec82f4ca08434b85d3329bad4bf301",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ed9744b33f834485b653da828cf72026",
            "value": "‚Äá160/160‚Äá[00:00&lt;00:00,‚Äá2077.03‚Äáexamples/s]"
          }
        },
        "52c005c417a84f9aa9604239dd62197e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b119211b51a4c1e847bf9c8e2fbd28c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ad5f99fdfdd942fab037f03f0f6c9107",
            "value": "Map:‚Äá100%"
          }
        },
        "673061059cb64053896079a0de7d3458": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c83dd832f784383aa67a7bac1c81722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02b048fcb50c451d89964ee93405cb93",
              "IPY_MODEL_f7d0f2a7a7df4589b2ff1ea0562b8ae5",
              "IPY_MODEL_ce5267e4265c49b08e066b4a5806e8cf"
            ],
            "layout": "IPY_MODEL_c7a0ffa43bfd45bb82a110520ef065c6"
          }
        },
        "6fc3ad2df4a64a2aaba0d4567e3a9277": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "908c81b282f64785b4fa729646b00c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f3f09b8e4bc411880e0d389c2043a88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad5f99fdfdd942fab037f03f0f6c9107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b45860c163b142189b6eb2eaacdcc1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6c6c1949f1b4bbc9624647c4640dff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52c005c417a84f9aa9604239dd62197e",
              "IPY_MODEL_f5926d20d7dc4bc5a784126f57867a59",
              "IPY_MODEL_49be6b548baf43c5bcd1091e2bfd56f6"
            ],
            "layout": "IPY_MODEL_ebfc99985a16470e8e49c9b788c18302"
          }
        },
        "c7a0ffa43bfd45bb82a110520ef065c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5267e4265c49b08e066b4a5806e8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f3f09b8e4bc411880e0d389c2043a88",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_da44f8d632714e7d89b2222686eafa5f",
            "value": "‚Äá1440/1440‚Äá[00:00&lt;00:00,‚Äá2639.12‚Äáexamples/s]"
          }
        },
        "da44f8d632714e7d89b2222686eafa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1ec82f4ca08434b85d3329bad4bf301": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebfc99985a16470e8e49c9b788c18302": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9744b33f834485b653da828cf72026": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef60343739754ac7a5d61a82749f2f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5926d20d7dc4bc5a784126f57867a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fc3ad2df4a64a2aaba0d4567e3a9277",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef60343739754ac7a5d61a82749f2f30",
            "value": 160
          }
        },
        "f7d0f2a7a7df4589b2ff1ea0562b8ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1173b7a21b3a4792bda7baa084fc5147",
            "max": 1440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b45860c163b142189b6eb2eaacdcc1f4",
            "value": 1440
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
