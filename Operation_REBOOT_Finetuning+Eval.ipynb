{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yl-PKGEcT4Rm",
   "metadata": {
    "id": "yl-PKGEcT4Rm"
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"800\" alt=\"Weights & Biases\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae8d5f",
   "metadata": {
    "id": "e6ae8d5f"
   },
   "source": [
    "# ü™ê Operation REBOOT: Mission Start\n",
    "\n",
    "Welcome, **Neural Architect**. The ship's AI core is down. Your job: fine-tune a foundational model with astrological Q&A data to restore its deep space reasoning abilities.\n",
    "\n",
    "**Your mission:**\n",
    "- Adjust dataset splits\n",
    "- Configure training arguments\n",
    "- Launch training and monitor with **Weights & Biases (W&B)**\n",
    "- Test and evaluate your fine-tuned model\n",
    "\n",
    "All systems go. Let's bring this vessel back online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z4hJWD_fQ_CA",
   "metadata": {
    "id": "Z4hJWD_fQ_CA"
   },
   "source": [
    "#### Install and Import  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udsFRNPeQ-WY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udsFRNPeQ-WY",
    "outputId": "20101bfd-1aa4-4f0f-ece2-0ed51d41503e"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate wandb trl bitsandbytes peft torchvision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Lcd7lpzkRH5a",
   "metadata": {
    "id": "Lcd7lpzkRH5a"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e039a1-4194-4ae4-8338-ed31bd097d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022b8ad",
   "metadata": {
    "id": "e022b8ad"
   },
   "source": [
    "## üîå Connect Neural Telemetry (W&B Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c79fd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5c79fd6",
    "outputId": "bc53d709-9eb1-419a-cdda-9bac5d1ff6ea"
   },
   "outputs": [],
   "source": [
    "# When prompted to authorize your\n",
    "# wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
    "# wandb: Paste an API key from your profile and hit enter:\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2252cad",
   "metadata": {
    "id": "b2252cad"
   },
   "outputs": [],
   "source": [
    "WANDB_PROJECT_NAME = \"FC-2025-FT-Workshop\"\n",
    "WANDB_ENTITY = None #Set your W&B Entity\n",
    "WORKSHOP_TEAM_NAME = None #Set to your Workshop Team Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67257b8",
   "metadata": {
    "id": "b67257b8"
   },
   "source": [
    "## üß™ Dataset Control Room\n",
    "Adjust the dataset splits and prepare the astrological QA dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8b93f",
   "metadata": {
    "id": "e3f8b93f"
   },
   "source": [
    "## üåé Initialize Experiment, Read Data, Split Data ‚òÑÔ∏è\n",
    "\n",
    "In this section, we:\n",
    "\n",
    "* Retrieve the Astros Dataset from [W&B Registry FC_FT_Workshop_Dataset collection](https://wandb.ai/orgs/FullyConnected-2025-Workshops/registry/dataset?selectionPath=fullyconnected-2025-workshops%2Fwandb-registry-dataset%2FFC_FT_Workshop_Dataset&view=versions) TODO: Update for final version\n",
    "* Load the Astros Dataset containing universe-related Q&A data.\n",
    "* Create prompts from the question/answer pairs & load into a pandas dataframe\n",
    "* Convert the pandas DataFrame into a Hugging Face Dataset.\n",
    "\n",
    "‚úÖ All the heavy lifting is done here automatically ‚Äî no manual setup needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9239842",
   "metadata": {
    "id": "f9239842"
   },
   "source": [
    "#### Let's prepare our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602a18a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "4602a18a",
    "outputId": "ec723b7f-2c25-4b41-a2b4-3eafc276fdae"
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize W&B run and download dataset\n",
    "print(\"Step 1: Downloading dataset from Weights & Biases...\")\n",
    "run = wandb.init(entity=WANDB_ENTITY,\n",
    "                 project=WANDB_PROJECT_NAME,\n",
    "                 job_type=\"data_retrieval\",\n",
    "                 name=\"fetch_astros_dataset\"\n",
    "                 )\n",
    "\n",
    "# Download the dataset artifact\n",
    "artifact = run.use_artifact('wandb-registry-dataset/FC_FT_Workshop_Dataset:v4', type='dataset')\n",
    "dataset_dir = artifact.download()\n",
    "run.finish()\n",
    "print(\"‚úÖ Dataset downloaded successfully!\")\n",
    "\n",
    "# Step 2: Load and prepare datasets\n",
    "df_train, training_dataset = load_and_prepare_dataset(dataset_dir, \"astro_dataset_train.jsonl\", \"training\") #look at the helper fuctions if you're interested in how we prepare the data\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Training examples: {len(df_train)}\")\n",
    "print(\"\\nExample prompt format:\")\n",
    "print(df_train['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120e194",
   "metadata": {},
   "source": [
    "### üåå Dataset Loaded Successfully!\n",
    "\n",
    "At this point, we've:\n",
    "* Retrieved the Astros Dataset artifact\n",
    "* Loaded it into a pandas DataFrame\n",
    "* Created prompt-style text for fine-tuning\n",
    "* Converted it into a Hugging Face Dataset for training\n",
    "\n",
    "‚ú® Feel free to pause and explore the data before moving forward!\n",
    "\n",
    "Exploring the dataset can help you:\n",
    "\n",
    "* Understand the kinds of questions and answers the model will learn from\n",
    "* Check for any strange patterns, formatting issues, or interesting insights\n",
    "* Discover Easter Eggs\n",
    "\n",
    "üõ°Ô∏è We've added soft error handling while loading, so if you accidentally modify the dataset file, you'll be warned if any loading issues happen.\n",
    "\n",
    "üëâ Quick Tip: You don't need to modify the dataset to proceed, but if you want to explore, you can run things like:\n",
    "\n",
    "```\n",
    "print(df_train.sample(5))\n",
    "print(df_train['question'].apply(len).describe())\n",
    "print(df_train['answer'].apply(len).describe())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045aafb",
   "metadata": {
    "id": "9045aafb"
   },
   "source": [
    "### üìò What Just Happened Space Cadet?\n",
    "You may have noticed these lines in the code:\n",
    "\n",
    "```\n",
    "run = wandb.init(...)\n",
    "artifact = run.use_artifact(...)\n",
    "```\n",
    "\n",
    "Let's break them down:\n",
    "\n",
    "```\n",
    "‚úÖ wandb.init(...)\n",
    "```\n",
    "\n",
    "This starts a new W&B run ‚Äî think of a run as a snapshot of a specific script execution. Every time you initialize a run, you're telling W&B:\n",
    "\n",
    "‚ÄúHey, I'm about to do something (like fetch data, train a model, evaluate results), track it for me.‚Äù\n",
    "\n",
    "It captures things like:\n",
    "\n",
    "* Your script's outputs\n",
    "* Hyperparameters and settings\n",
    "* Metrics over time\n",
    "* System usage\n",
    "* Artifacts you use or create\n",
    "\n",
    "In this step, we named the run `fetch_astros_dataset` and gave it a `job_type` of `data_retrieval` to describe what kind of task it's doing.\n",
    "\n",
    "```\n",
    "üì¶ run.use_artifact(...)\n",
    "```\n",
    "\n",
    "Artifacts are W&B's way of tracking versioned data and models. When you call:\n",
    "\n",
    "```\n",
    "artifact = run.use_artifact('wandb-registry-dataset/FC_FT_Workshop_Dataset:v4', type='dataset')\n",
    "```\n",
    "\n",
    "You're saying:\n",
    "\n",
    "‚ÄúI want to use a specific version (v4) of a dataset that lives in the W&B artifact registry.‚Äù\n",
    "\n",
    "This:\n",
    "\n",
    "* Downloads the dataset to your machine\n",
    "* Logs to W&B that this run used that version of the dataset\n",
    "* Enables full traceability of which data was used during which run\n",
    "\n",
    "Together, these steps help ensure your experiments are reproducible, shareable, and traceable ‚Äî key ingredients for good FineTuning Practice.\n",
    "\n",
    "Want to learn more about Weights & Biases Experiments, checkout the following resources!\n",
    "* [W&B Experiment Tracking](https://docs.wandb.ai/guides/track/)\n",
    "* [W&B Artifacts](https://docs.wandb.ai/guides/artifacts/)\n",
    "\n",
    "When you're ready, move on to loading the model and tokenizing the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001ed2e",
   "metadata": {
    "id": "7001ed2e"
   },
   "source": [
    "## üß† Model Vault: Load & Configure the Neural Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5caa9d0",
   "metadata": {
    "id": "e5caa9d0"
   },
   "source": [
    "## üöÄ Load Pretrained Model and Prepare Dataset for Fine-Tuning üå†\n",
    "In this section, we:\n",
    "\n",
    "* Retrieve the Model to Finetune from [W&B Registry FC_FT_Workshop_Model](https://wandb.ai/orgs/FullyConnected-2025-Workshops/registry/model?selectionPath=fullyconnected-2025-workshops%2Fwandb-registry-model%2FFC_FT_Workshop_Model&view=versions) TODO: Update for final version\n",
    "* Select and load a pretrained language model and its tokenizer from Hugging Face.\n",
    "* Format the Astros prompts into tokenized input IDs the model can understand.\n",
    "* Apply padding and truncation to keep sequence lengths manageable.\n",
    "* Split the tokenized dataset into training and validation sets (90% train / 10% validation).\n",
    "\n",
    "‚úÖ All the setup for model loading, tokenization, and data splitting is handled for you ‚Äî no manual steps required!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sblwkf4QihQZ",
   "metadata": {
    "id": "Sblwkf4QihQZ"
   },
   "source": [
    "### Select Model\n",
    "You will be prompted to select one of the following models\n",
    "\n",
    "*   Option 1: [falcon-rw-1b](https://huggingface.co/tiiuae/falcon-rw-1b)\n",
    "*   Option 2: [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WLBfMTo6h5sr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "WLBfMTo6h5sr",
    "outputId": "f49cfa52-bd51-4448-a8d6-1682dd608afe"
   },
   "outputs": [],
   "source": [
    "model, tokenizer, model_name = get_model_from_wandb(WANDB_ENTITY, WANDB_PROJECT_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26597b6b",
   "metadata": {
    "id": "26597b6b"
   },
   "source": [
    "‚úÖ Model and tokenizer successfully loaded from the artifact!\n",
    "\n",
    "Next, we'll make a few adjustments to ensure the model handles padding correctly,\n",
    "and then prepare our dataset for training by tokenizing the input prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff0e01",
   "metadata": {
    "id": "1dff0e01"
   },
   "source": [
    "## üîÑ Tokenize & Split: Format Data for Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6780204",
   "metadata": {
    "id": "c6780204"
   },
   "source": [
    "#### Load the datasets\n",
    "\n",
    "You can modify how our training data is passed to our training script to finetune the model. Make sure to analyze the data so you can select an appropriate **Sample Size** and  **Train/Test split** for the finetuning process.\n",
    "\n",
    "TO-DO: Update the values below for your first run. You can come back and try it with new values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61faf47c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "6c83dd832f784383aa67a7bac1c81722",
      "02b048fcb50c451d89964ee93405cb93",
      "f7d0f2a7a7df4589b2ff1ea0562b8ae5",
      "ce5267e4265c49b08e066b4a5806e8cf",
      "c7a0ffa43bfd45bb82a110520ef065c6",
      "673061059cb64053896079a0de7d3458",
      "908c81b282f64785b4fa729646b00c98",
      "1173b7a21b3a4792bda7baa084fc5147",
      "b45860c163b142189b6eb2eaacdcc1f4",
      "9f3f09b8e4bc411880e0d389c2043a88",
      "da44f8d632714e7d89b2222686eafa5f",
      "c6c6c1949f1b4bbc9624647c4640dff9",
      "52c005c417a84f9aa9604239dd62197e",
      "f5926d20d7dc4bc5a784126f57867a59",
      "49be6b548baf43c5bcd1091e2bfd56f6",
      "ebfc99985a16470e8e49c9b788c18302",
      "3b119211b51a4c1e847bf9c8e2fbd28c",
      "ad5f99fdfdd942fab037f03f0f6c9107",
      "6fc3ad2df4a64a2aaba0d4567e3a9277",
      "ef60343739754ac7a5d61a82749f2f30",
      "e1ec82f4ca08434b85d3329bad4bf301",
      "ed9744b33f834485b653da828cf72026"
     ]
    },
    "id": "61faf47c",
    "outputId": "a6bde064-b7fb-4d79-c860-14a63d123c99"
   },
   "outputs": [],
   "source": [
    "training_sample = training_dataset.shuffle(seed=42).select(range(100)) # chosse between 100 and 1600 samples\n",
    "\n",
    "train_test_split = 0.1 # choose a float value between 0 and 1\n",
    "\n",
    "train_dataset, eval_dataset = tokenized_train_test(training_dataset, train_test_split, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b86f5",
   "metadata": {
    "id": "e08b86f5"
   },
   "source": [
    "## ‚öôÔ∏è Training Command Center\n",
    "Set training arguments to guide your model's learning trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65c01f",
   "metadata": {
    "id": "5e65c01f"
   },
   "source": [
    "## üõ∞Ô∏è Training Arguments (Where You Fine-Tune Settings) üåô\n",
    "\n",
    "This is where you'll do most of your experimentation! üéØ\n",
    "\n",
    "The `TrainingArguments` object controls how your model is fine-tuned, including:\n",
    "\n",
    "* Batch size\n",
    "* Number of epochs\n",
    "* Learning rate\n",
    "* Warmup steps\n",
    "* Mixed precision (fp16) for faster training\n",
    "* Checkpoint saving\n",
    "* Reporting to Weights & Biases\n",
    "\n",
    "You can modify the hyperparameters here to see how different settings impact model performance.\n",
    "\n",
    "üî• **Pro Tips for Tuning:**\n",
    "\n",
    "- If your model is training too slowly or crashing with memory errors, try lowering the `per_device_train_batch_size` or enabling `auto_find_batch_size`.\n",
    "\n",
    "- Notice overfitting early? You might be training too long or with too high a learning rate. Think about adjusting `num_train_epochs` or using `eval_strategy=\"steps\"` for finer control.\n",
    "\n",
    "- Seeing noisy validation loss? `gradient_accumulation_steps` lets you \"fake\" a larger batch size without blowing up your GPU.\n",
    "\n",
    "- Feel like your model isn't converging? Try experimenting with `lr_scheduler_type`. `cosine` is smooth, but `linear` or `constant` can behave differently.\n",
    "\n",
    "- Wondering what your model is *actually learning*? Keep an eye on your W&B run dashboard ‚Äî watch loss curves, learning rate progression, and system utilization.\n",
    "\n",
    "- Training is slow but eval is fast? That's expected. Training includes backward pass + optimization, while eval is forward-only. Don't panic!\n",
    "\n",
    "üëÄ Keep tweaking. Your best config probably won't be your first ‚Äî and that's part of the fun!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "013c0d65",
   "metadata": {
    "id": "013c0d65"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=f\"fine-tuning-{model_name}-qlora\",\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3, #start with 3 and go upto 10 epochs for better results\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    dataloader_num_workers=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True, #Choose to store the full forward-pass activations in GPU RAM\n",
    "    group_by_length=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    optim=\"adamw_torch\", #See https://huggingface.co/docs/transformers/v4.51.3/en/perf_train_gpu_one#optimizers\n",
    "    learning_rate=2e-3,\n",
    "    lr_scheduler_type=\"cosine\", # See https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.SchedulerType\n",
    "    auto_find_batch_size=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_strategy=\"steps\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcc2dd",
   "metadata": {
    "id": "a9dcc2dd"
   },
   "source": [
    "## üõ∞Ô∏è Engage Training Tracker\n",
    "Launch the model and track training live with W&B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faff1ee",
   "metadata": {
    "id": "7faff1ee"
   },
   "source": [
    "## üî≠ Initialize Trainer, Train, and Save üåé\n",
    "\n",
    "In this final section:\n",
    "\n",
    "* We initialize the Trainer with:\n",
    "  * The model\n",
    "  * The datasets (train/test splits)\n",
    "  * The training arguments\n",
    "  * A data collator for language modeling\n",
    "\n",
    "* We start training by calling trainer.train().\n",
    "* We save the fine-tuned model and tokenizer locally.\n",
    "* We finish the W&B run to close the logging cleanly.\n",
    "\n",
    "üß† Reminder: After training finishes, your fine-tuned model will be available in your local runtime ‚Äî you can upload it back to W&B or Hugging Face later!\n",
    "\n",
    "üö® Training Ahead: Be ready for 10-15 min runtimes with the default configs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc5c7d",
   "metadata": {
    "id": "f7bc5c7d"
   },
   "source": [
    "## ‚öô Now we kick off the training process ‚öô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3aabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure model for training\n",
    "model.config.use_cache = False  # Disable cache during training\n",
    "\n",
    "# Set label names for PEFT model\n",
    "model.config.label_names = [\"labels\"]\n",
    "\n",
    "# Initialize trainer with modified configuration\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=8  # Add padding to multiple of 8 for better performance\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    #compute_metrics=compute_perpexity, #If monitoring additional metric(s) can set this argument accordingly to your compute functions\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing with the new format\n",
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "def ignite_training():\n",
    "    from accelerate.state import AcceleratorState\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "    AcceleratorState._reset_state()\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # Launch training\n",
    "    launch_sequence(training_args)\n",
    "    time.sleep(5)\n",
    "    print(\"üöÄ Starting training process...\")\n",
    "\n",
    "    run = wandb.init(\n",
    "        entity=WANDB_ENTITY,\n",
    "        project=WANDB_PROJECT_NAME,\n",
    "        job_type=\"finetuning_job\",\n",
    "        name=f\"fine_tune_{model_name}\"\n",
    "    )\n",
    "\n",
    "    # Recreate Trainer only now\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8\n",
    "        ),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train()\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    return train_output\n",
    "\n",
    "training_output = ignite_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4c42f",
   "metadata": {
    "id": "00f4c42f"
   },
   "source": [
    "## üíæ Save & Upload\n",
    "Preserve your fine-tuned model as a W&B artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e32a1",
   "metadata": {
    "id": "6b8e32a1"
   },
   "source": [
    "We will now save this model to W&B. You will need the artifact refernce for this model to get points for completing the fine tuning section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5891f1",
   "metadata": {
    "id": "1c5891f1"
   },
   "source": [
    "Tracking your model in W&B can be really helpful:\n",
    "\n",
    "- You can now share this model with your team and beyond\n",
    "- W&B creates a lineage map of your model so you can see the full model lifecycle dataset->training->final state\n",
    "\n",
    "Tracking an artifact's lineage has several key benefits:\n",
    "\n",
    "* Reproducibility: By tracking the lineage of all artifacts, teams can reproduce experiments, models, and results, which is essential for debugging, experimentation, and validating machine learning models.\n",
    "\n",
    "* Version Control: Artifact lineage involves versioning artifacts and tracking their changes over time. This allows teams to roll back to previous versions of data or models if needed.\n",
    "\n",
    "* Auditing: Having a detailed history of the artifacts and their transformations enables organizations to comply with regulatory and governance requirements.\n",
    "\n",
    "* Collaboration and Knowledge Sharing: Artifact lineage facilitates better collaboration among team members by providing a clear record of attempts as well as what worked, and what didn‚Äôt. This helps in avoiding duplication of efforts and accelerates the development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428cdbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "f428cdbc",
    "outputId": "4798665a-7613-4a82-eed3-2b67ba057f96"
   },
   "outputs": [],
   "source": [
    "#Saving and uploading best model - working draft\n",
    "trainer.save_model(f\"./best_model/{type(model.base_model.model).__name__}\")\n",
    "tokenizer.save_pretrained(f\"./best_model/{type(model.base_model.model).__name__}\")\n",
    "\n",
    "run = wandb.init(project=WANDB_PROJECT_NAME,\n",
    "                 entity=WANDB_ENTITY,\n",
    "                 job_type=\"FT-Workshop-Finetuning-Best-Model-Upload\",\n",
    "                 name=\"FT-Best-Model-Upload\")\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=f\"{WORKSHOP_TEAM_NAME}-ft-best-model-{type(model.base_model.model).__name__}\",\n",
    "    type=\"model\",\n",
    "    description=\"\"\"Best FineTuned model from the Astros-FT-Workshop.\"\"\"\n",
    ")\n",
    "\n",
    "artifact.add_dir(local_path=\"./best_model\")\n",
    "\n",
    "logged_artifact = run.log_artifact(artifact)\n",
    "\n",
    "run.link_artifact(\n",
    "  artifact=logged_artifact,\n",
    "  target_path=\"wandb-registry-model/FC_FT_Workshop_FineTuned_Models\"\n",
    ")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90dd70",
   "metadata": {
    "id": "2e90dd70"
   },
   "source": [
    "## ‚úÖ Mission Checkpoint: Model Finetuned\n",
    "\n",
    "Congratulations, Architect! You've:\n",
    "- Loaded and prepped your training dataset ‚úÖ\n",
    "- Configured a foundational model ‚úÖ\n",
    "- Finetuned it with parameter-efficient methods ‚úÖ\n",
    "- Logged your training runs and saved the final model to Weights & Biases ‚úÖ\n",
    "\n",
    "Your model is now part of your mission's neural infrastructure.\n",
    "\n",
    "Next, we prepare to test and evaluate. But first, a quick system check..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e953f25",
   "metadata": {
    "id": "8e953f25"
   },
   "source": [
    "## üß∞ Systems Maintenance Bay: Utilities\n",
    "\n",
    "Before testing, it's wise to flush memory and check your hardware status. Use these utilities to prepare the environment.\n",
    "\n",
    "Just like a good engineer, make sure the ship's neural bays are cleared and ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761f2d7",
   "metadata": {
    "id": "b761f2d7"
   },
   "source": [
    "## Utilities üß∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f922d",
   "metadata": {
    "id": "233f922d"
   },
   "outputs": [],
   "source": [
    "# -- Flush out GPU memory - when required - may require restarting the notebook\n",
    "import gc, torch\n",
    "\n",
    "try:\n",
    "    del trainer\n",
    "except: print(\"cannot release memory\")\n",
    "try:\n",
    "    del model\n",
    "except: print(\"cannot release memory\")\n",
    "try:\n",
    "    del tokenizer\n",
    "except: print(\"cannot release memory\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767a0d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a767a0d9",
    "outputId": "8e81b801-5015-4fbd-a287-5e0966f7e6f5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f71a7",
   "metadata": {
    "id": "5d0f71a7"
   },
   "source": [
    "## üß™ Testing the Neural Core\n",
    "\n",
    "Now that your model is trained and uploaded, it‚Äôs time to test your ship‚Äôs new neural core.\n",
    "\n",
    "You‚Äôll load the fine-tuned model and run test prompts to ensure it responds with precision and depth‚Äîcritical for deep-space operations.\n",
    "\n",
    "We‚Äôve equipped you with a call function wrapped in `Weave`, our GenAI interface and telemetry layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60a175",
   "metadata": {
    "id": "bf60a175"
   },
   "source": [
    "# üîß Testing our model ü™õ\n",
    "\n",
    "Let's start by creating some helper functions to load and call the model we just trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdfa6c",
   "metadata": {
    "id": "eefdfa6c"
   },
   "source": [
    "Since we created an adapter during the finetuning process, our load model function loads the original model along with our adapter using PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8abc80",
   "metadata": {
    "id": "0e8abc80"
   },
   "source": [
    "## üõ∞Ô∏è Introducing Weave: Your AI Telemetry and Evaluation Suite\n",
    "\n",
    "**Weave** is Weights & Biases‚Äô next-gen platform for tracking, evaluating, and visualizing GenAI applications.\n",
    "\n",
    "In REBOOT, you'll use Weave to:\n",
    "- Log and score model generations\n",
    "- Run structured evaluations on Q&A performance\n",
    "- Compare outputs with reference answers\n",
    "\n",
    "This enables you to **quantitatively assess** how mission-ready your model is.\n",
    "\n",
    "Let‚Äôs initialize Weave and plug it into your finetuned system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7348cba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7348cba",
    "outputId": "8e986fcd-5e33-44ea-cee8-f34ede1d011c"
   },
   "outputs": [],
   "source": [
    "!pip install weave \"weave[scorers]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fd38a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "309fd38a",
    "outputId": "4155a61d-a420-4e3a-e7b4-ce506a9ed1f9"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2de66",
   "metadata": {
    "id": "57f2de66"
   },
   "source": [
    "#### Helper functions to load our local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572e137",
   "metadata": {
    "id": "1572e137"
   },
   "outputs": [],
   "source": [
    "#helper\n",
    "def load_finetuned_model(adapter_dir, base_model_dir):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\n",
    "\n",
    "    # Load base model\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # Load model with quantization\n",
    "    print(\"Loading model with 4-bit quantization...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        Path(base_model_dir),\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        )\n",
    "    # Load fine-tuned adapter\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F4vH2PsqWsDD",
   "metadata": {
    "id": "F4vH2PsqWsDD"
   },
   "source": [
    "### Calling our Local Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bb674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "473bb674",
    "outputId": "c4eb4471-34d1-4095-f776-242bdb4231ab"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def call_model(question: str) -> str:\n",
    "    \"\"\"Generate an answer from your Local LLM given a prompt.\"\"\"\n",
    "\n",
    "    system_prompt = \"You are an expert in astrophysics. Please provide a concise and truthful answer to the following question:\"\n",
    "    prompt = system_prompt + \"\\n\\n\" + question + \"\\nAnswer:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=150, do_sample=False, eos_token_id=tokenizer.eos_token_id, pad_token_id=model.config.eos_token_id)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).replace(prompt, '').strip(),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25f02a",
   "metadata": {
    "id": "bd25f02a"
   },
   "source": [
    "## üìä Final Check: Evaluation Protocols\n",
    "\n",
    "Your neural core is active‚Äîbut is it mission-grade?\n",
    "\n",
    "Use this section to:\n",
    "- Load an evaluation dataset\n",
    "- Score model responses using embedding similarity\n",
    "- Track performance with W&B + Weave\n",
    "\n",
    "**Evaluation is critical** before deployment‚Äîit ensures your model‚Äôs reasoning is aligned with mission parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b39f03",
   "metadata": {
    "id": "a4b39f03"
   },
   "source": [
    "# Evaluating our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a6f0f",
   "metadata": {
    "id": "700a6f0f"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82b834",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf82b834",
    "outputId": "5119b1d1-e3b7-47c2-ea1a-21105b2a7ca3"
   },
   "outputs": [],
   "source": [
    "base_model_dir = \"./models/TinyLlama_v1\" # Path to base model - modify accordingly to fine_tuned_model/<TinyLlama_v1 or falcon-rw-1b_v0>\n",
    "adapter_dir = f\"./best_model/{type(model.base_model.model).__name__}\" #add path to adapter dir\n",
    "\n",
    "tokenizer, model = load_finetuned_model(adapter_dir, base_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf9ddb",
   "metadata": {
    "id": "53cf9ddb"
   },
   "source": [
    "## Get Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db236752",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db236752",
    "outputId": "2123d874-4710-4544-c9bf-aa2909dab1c2"
   },
   "outputs": [],
   "source": [
    "weave.init('fc25-wandb-admins/uncategorized')\n",
    "eval_dataset_public_v0 = weave.ref('eval_dataset_public:v0').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d915b61",
   "metadata": {
    "id": "2d915b61"
   },
   "source": [
    "## Test the model with a sample from our eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f64721",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79f64721",
    "outputId": "71aabdb8-48ee-4498-da9e-453bacd5dbd3"
   },
   "outputs": [],
   "source": [
    "question = eval_dataset_public_v0[20]['question']\n",
    "answer = call_model(question)\n",
    "\n",
    "print(\"üõ∞Ô∏è  Incoming Transmission ‚Äî Mission Q&A\\n\")\n",
    "print(f\"üß† Question:\\n{question}\\n\")\n",
    "print(f\"ü§ñ Model Response:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1d927",
   "metadata": {
    "id": "5bf1d927"
   },
   "source": [
    "## Setup eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cfaee",
   "metadata": {
    "id": "ff0cfaee"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from weave.scorers import EmbeddingSimilarityScorer\n",
    "similarity_scorer = EmbeddingSimilarityScorer(\n",
    "    model_id=\"openai/text-embedding-3-small\",  # will need to update this to bedrock's titan models\n",
    "    threshold=0.7\n",
    ")\n",
    "\n",
    "similarity_scorer.column_map = {\n",
    "    \"output\": \"model_output\",  # Your model's response\n",
    "    \"target\": \"answer\", # The reference response (expected)\n",
    "    \"kwargs\": \"question\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5479015",
   "metadata": {
    "id": "a5479015"
   },
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset_public_v0.rows[20:25] # select samples to run evalaution against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ef801",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "533ef801",
    "outputId": "55475355-aecc-4822-c3fc-9a085ff202e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')# will remove this once we move to bedrock\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    evaluation_name = f\"{WORKSHOP_TEAM_NAME}_qna_eval\",\n",
    "    dataset=eval_dataset, scorers=[similarity_scorer],\n",
    "    name=\"model_qna_eval\"\n",
    "   )\n",
    "\n",
    "print(await(evaluation.evaluate(call_model)) )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02b048fcb50c451d89964ee93405cb93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_673061059cb64053896079a0de7d3458",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_908c81b282f64785b4fa729646b00c98",
      "value": "Map:‚Äá100%"
     }
    },
    "1173b7a21b3a4792bda7baa084fc5147": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b119211b51a4c1e847bf9c8e2fbd28c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49be6b548baf43c5bcd1091e2bfd56f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1ec82f4ca08434b85d3329bad4bf301",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ed9744b33f834485b653da828cf72026",
      "value": "‚Äá160/160‚Äá[00:00&lt;00:00,‚Äá2077.03‚Äáexamples/s]"
     }
    },
    "52c005c417a84f9aa9604239dd62197e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b119211b51a4c1e847bf9c8e2fbd28c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ad5f99fdfdd942fab037f03f0f6c9107",
      "value": "Map:‚Äá100%"
     }
    },
    "673061059cb64053896079a0de7d3458": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c83dd832f784383aa67a7bac1c81722": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02b048fcb50c451d89964ee93405cb93",
       "IPY_MODEL_f7d0f2a7a7df4589b2ff1ea0562b8ae5",
       "IPY_MODEL_ce5267e4265c49b08e066b4a5806e8cf"
      ],
      "layout": "IPY_MODEL_c7a0ffa43bfd45bb82a110520ef065c6"
     }
    },
    "6fc3ad2df4a64a2aaba0d4567e3a9277": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "908c81b282f64785b4fa729646b00c98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f3f09b8e4bc411880e0d389c2043a88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad5f99fdfdd942fab037f03f0f6c9107": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b45860c163b142189b6eb2eaacdcc1f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c6c6c1949f1b4bbc9624647c4640dff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52c005c417a84f9aa9604239dd62197e",
       "IPY_MODEL_f5926d20d7dc4bc5a784126f57867a59",
       "IPY_MODEL_49be6b548baf43c5bcd1091e2bfd56f6"
      ],
      "layout": "IPY_MODEL_ebfc99985a16470e8e49c9b788c18302"
     }
    },
    "c7a0ffa43bfd45bb82a110520ef065c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce5267e4265c49b08e066b4a5806e8cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f3f09b8e4bc411880e0d389c2043a88",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_da44f8d632714e7d89b2222686eafa5f",
      "value": "‚Äá1440/1440‚Äá[00:00&lt;00:00,‚Äá2639.12‚Äáexamples/s]"
     }
    },
    "da44f8d632714e7d89b2222686eafa5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1ec82f4ca08434b85d3329bad4bf301": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebfc99985a16470e8e49c9b788c18302": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed9744b33f834485b653da828cf72026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef60343739754ac7a5d61a82749f2f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f5926d20d7dc4bc5a784126f57867a59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fc3ad2df4a64a2aaba0d4567e3a9277",
      "max": 160,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef60343739754ac7a5d61a82749f2f30",
      "value": 160
     }
    },
    "f7d0f2a7a7df4589b2ff1ea0562b8ae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1173b7a21b3a4792bda7baa084fc5147",
      "max": 1440,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b45860c163b142189b6eb2eaacdcc1f4",
      "value": 1440
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
